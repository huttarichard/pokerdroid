# Superhuman AI for multiplayer poker 

Noam Brown ${ }^{1,2 *}$ and Tuomas Sandholm ${ }^{1,3,4,5 *}$<br>${ }^{1}$ Computer Science Department, Carnegie Mellon University Pittsburgh, PA 15213, USA. ${ }^{2}$ Facebook AI Research New York, NY 10003, USA. ${ }^{3}$ Strategic Machine, Inc. Pittsburgh, PA 15213, USA. ${ }^{4}$ Strategy Robot, Inc. Pittsburgh, PA 15213, USA. ${ }^{5}$ Optimized Markets, Inc. Pittsburgh, PA 15213, USA.<br>*Corresponding author. E-mail: noamb@cs.cmu.edu (N.B.); sandholm@cs.cmu.edu (T.S.)

In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.

Poker has served as a challenge problem for the fields of artificial intelligence (AI) and game theory for decades (1). In fact, the foundational papers on game theory used poker to illustrate their concepts $(2,3)$. The reason for this choice is simple: no other popular recreational game captures the challenges of hidden information as effectively and as elegantly as poker. Although poker has been useful as a benchmark for new AI and game-theoretic techniques, the challenge of hidden information in strategic settings is not limited to recreational games. The equilibrium concepts of von Neumann and Nash have been applied to many realworld challenges such as auctions, cybersecurity, and pricing.

The past two decades have witnessed rapid progress in the ability of AI systems to play increasingly complex forms of poker (4-6). However, all prior breakthroughs have been limited to settings involving only two players. Developing a superhuman AI for multiplayer poker was the widelyrecognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold'em poker, the most commonly played poker format in the world.

## Theoretical and practical challenges of multiplayer games

AI systems have reached superhuman performance in games such as checkers (7), chess (8), two-player limit poker (4), Go (9), and two-player no-limit poker (6). All of these involve only two players and are zero-sum games (meaning that whatever one player wins, the other player loses). Every one of those superhuman AI systems was generated by attempting to approximate a Nash equilibrium strategy rather than by, for example, trying to detect and exploit weaknesses in the opponent. A Nash equilibrium is a list of strategies, one for each player, in which no player can improve by deviating to a different strategy. Nash equilibria have been proven to
exist in all finite games, and many infinite games, though finding an equilibrium may be difficult.

Two-player zero-sum games are a special class of games in which Nash equilibria also have an extremely useful additional property: any player who chooses to use a Nash equilibrium is guaranteed to not lose in expectation no matter what the opponent does (as long as one side does not have an intrinsic advantage under the game rules, or the players alternate sides). In other words, a Nash equilibrium strategy is unbeatable in two-player zero-sum games that satisfy the above criteria. For this reason, to "solve" a two-player zerosum game means to find an exact Nash equilibrium. For example, the Nash equilibrium strategy for Rock-Paper-Scissors is to randomly pick Rock, Paper, or Scissors with equal probability. Against such a strategy, the best that an opponent can do in expectation is tie (10). In this simple case, playing the Nash equilibrium also guarantees that the player will not win in expectation. However, in more complex games even determining how to tie against a Nash equilibrium may be difficult; if the opponent ever chooses suboptimal actions, then playing the Nash equilibrium will indeed result in victory in expectation.

In principle, playing the Nash equilibrium can be combined with opponent exploitation by initially playing the equilibrium strategy and then over time shifting to a strategy that exploits the opponent's observed weaknesses (for example, by switching to always playing Paper against an opponent that always plays Rock) (11). However, except in certain restricted ways (12), shifting to an exploitative non-equilibrium strategy opens oneself up to exploitation because the opponent could also change strategies at any moment. Additionally, existing techniques for opponent exploitation require too many samples to be competitive with human ability outside of small games. Pluribus plays a fixed strategy that does not adapt to the observed tendencies of the opponents.

Although a Nash equilibrium strategy is guaranteed to exist in any finite game, efficient algorithms for finding one are only proven to exist for special classes of games, among which two-player zero-sum games are the most prominent. No pol-ynomial-time algorithm is known for finding a Nash equilibrium in two-player non-zero-sum games, and the existence of one would have sweeping surprising implications in computational complexity theory (13, 14). Finding a Nash equilibrium in zero-sum games with three or more players is at least as hard (because a dummy player can be added to the twoplayer game to make it a three-player zero-sum game). Even approximating a Nash equilibrium is hard (except in special cases) in theory (15) and in games with more than two players, even the best complete algorithm can only address games with a handful of possible strategies per player (16). Moreover, even if a Nash equilibrium could be computed efficiently in a game with more than two players, it is not clear that playing such an equilibrium strategy would be wise. If each player in such a game independently computes and plays a Nash equilibrium, the list of strategies that they play (one strategy per player) may not be a Nash equilibrium and players might have an incentive to deviate to a different strategy. One example of this is the Lemonade Stand Game (17), illustrated in Fig. 1, in which each player simultaneously picks a point on a ring and wants to be as far away as possible from any other player. The Nash equilibrium is for all players to be spaced uniformly along the ring, but there are infinitely many ways this can be accomplished and therefore infinitely many Nash equilibria. If each player independently computes one of those equilibria, the joint strategy is unlikely to result in all players being spaced uniformly along the ring. Two-player zero-sum games are a special case where even if the players independently compute and select Nash equilibria, the list of strategies is still a Nash equilibrium.

The shortcomings of Nash equilibria outside of two-player zero-sum games, and the failure of any other game-theoretic solution concept to convincingly overcome them, have raised the question of what the right goal should even be in such games. In the case of six-player poker, we take the viewpoint that our goal should not be a specific game-theoretic solution concept, but rather to create an AI that empirically consistently defeats human opponents, including elite human professionals.

The algorithms we used to construct Pluribus, discussed in the next two sections, are not guaranteed to converge to a Nash equilibrium outside of two-player zero-sum games. Nevertheless, we observe that Pluribus plays a strong strategy in multiplayer poker that is capable of consistently defeating elite human professionals. This shows that even though the techniques do not have known strong theoretical guarantees on performance outside of the two-player zero-sum setting, they are nevertheless capable of producing superhuman
strategies in a wider class of strategic settings.

## Description of Pluribus

The core of Pluribus's strategy was computed via self play, in which the AI plays against copies of itself, without any data of human or prior AI play used as input. The AI starts from scratch by playing randomly, and gradually improves as it determines which actions, and which probability distribution over those actions, lead to better outcomes against earlier versions of its strategy. Forms of self play have previously been used to generate powerful AIs in two-player zero-sum games such as backgammon (18), Go (9, 19), Dota 2 (20), StarCraft 2 (21), and two-player poker (4-6), though the precise algorithms that were used have varied widely. Although it is easy to construct toy games with more than two players in which commonly-used self-play algorithms fail to converge to a meaningful solution (22), in practice self play has nevertheless been shown to do reasonably well in some games with more than two players (23).

Pluribus's self play produces a strategy for the entire game offline, which we refer to as the blueprint strategy. Then during actual play against opponents, Pluribus improves upon the blueprint strategy by searching for a better strategy in real time for the situations it finds itself in during the game. In subsections below, we discuss both of those phases in detail, but first we discuss abstraction, forms of which are used in both phases to make them scalable.

## Abstractionfor large imperfect-information games

There are far too many decision points in no-limit Texas hold'em to reason about individually. To reduce the complexity of the game, we eliminate some actions from consideration and also bucket similar decision points together in a process called abstraction (24, 25). After abstraction, the bucketed decision points are treated as identical. We use two kinds of abstraction in Pluribus: action abstraction and information abstraction.

Action abstraction reduces the number of different actions the AI needs to consider. No-limit Texas hold'em normally allows any whole-dollar bet between $\$ 100$ and $\$ 10,000$. However, in practice there is little difference between betting $\$ 200$ and betting $\$ 201$. To reduce the complexity of forming a strategy, Pluribus only considers a few different bet sizes at any given decision point. The exact number of bets it considers varies between one and 14 depending on the situation. Although Pluribus can limit itself to only betting one of a few different sizes between $\$ 100$ and $\$ 10,000$, when actually playing no-limit poker, the opponents are not constrained to those few options. What happens if an opponent bets $\$ 150$ while Pluribus has only been trained to consider bets of $\$ 100$ or $\$ 200$ ? Generally, Pluribus will rely on its search algorithm, described in a later section, to compute a response in real
time to such "off-tree" actions.
The other form of abstraction we use in Pluribus is information abstraction, in which decision points that are similar in terms of what information has been revealed (in poker, the player's cards and revealed board cards) are bucketed together and treated identically (26-28). For example, a tenhigh straight and a nine-high straight are distinct hands, but are nevertheless strategically similar. Pluribus may bucket these hands together and treat them identically, thereby reducing the number of distinct situations for which it needs to determine a strategy. Information abstraction drastically reduces the complexity of the game, but may wash away subtle differences that are important for superhuman performance. Therefore, during actual play against humans, Pluribus uses information abstraction only to reason about situations on future betting rounds, never the betting round it is actually in. Information abstraction is also applied during offline self play.

## Self play via improved Monte Carlo counterfactual regret minimization

The blueprint strategy in Pluribus was computed using a variant of counterfactual regret minimization (CFR) (29). CFR is an iterative self-play algorithm in which the AI starts by playing completely at random but gradually improves by learning to beat earlier versions of itself. Every competitive Texas hold'em AI for at least the past six years has computed its strategy using some variant of CFR (4-6, 23, 28, 30-34). We use a form of Monte Carlo CFR (MCCFR) that samples actions in the game tree rather than traversing the entire game tree on each iteration (33, 35-37).

On each iteration of the algorithm, MCCFR designates one player as the traverser whose current strategy is updated on the iteration. At the start of the iteration, MCCFR simulates a hand of poker based on the current strategy of all players (which is initially completely random). Once the simulated hand is completed, the AI reviews each decision that was made by the traverser and investigates how much better or worse it would have done by choosing the other available actions instead. Next, the AI reviews each hypothetical decision that would have been made following those other available actions and investigates how much better it would have done by choosing the other available actions, and so on. This traversal of the game tree is illustrated in Fig. 2. Exploring other hypothetical outcomes is possible because the AI knows each player's strategy for the iteration, and can therefore simulate what would have happened had some other action been chosen instead. This counterfactual reasoning is one of the features that distinguishes CFR from other self-play algorithms that have been deployed in domains such as Go (9), Dota 2 (20), and StarCraft 2 (21).

The difference between what the traverser would have
received for choosing an action versus what the traverser actually achieved (in expectation) on the iteration is added to the counterfactual regret for the action. Counterfactual regret represents how much the traverser regrets having not chosen that action in previous iterations. At the end of the iteration, the traverser's strategy is updated so that actions with higher counterfactual regret are chosen with higher probability.

For two-player zero-sum games, CFR guarantees that the average strategy played over all iterations converges to a Nash equilibrium, but convergence to a Nash equilibrium is not guaranteed outside of two-player zero-sum games. Nevertheless, CFR guarantees in all finite games that all counterfactual regrets grow sublinearly in the number of iterations. This, in turn, guarantees in the limit that the average performance of CFR on each iteration that was played matches the average performance of the best single fixed strategy in hindsight. CFR is also proven to eliminate iteratively strictly dominated actions in all finite games (23).

Because the difference between counterfactual value and expected value is added to counterfactual regret rather than replacing it, the first iteration in which the agent played completely randomly (which is typically a very bad strategy) still influences the counterfactual regrets, and therefore the strategy that is played, for iterations far into the future. In the vanilla form of CFR, the influence of this first iteration decays at a rate of $\frac{1}{T}$, where $T$ is the number of iterations played. In order to more quickly decay the influence of these early "bad" iterations, Pluribus uses a recent form of CFR called Linear CFR (38) in early iterations. (We stop the discounting after that because the time cost of doing the multiplications with the discount factor is not worth the benefit later on.) Linear CFR assigns a weight of $T$ to the regret contributions of iteration $T$. Therefore, the influence of the first iteration decays at a rate of $\frac{1}{\sum_{t=1}^{T} t}=\frac{2}{T(T+1)}$. This leads to the strategy improving significantly more quickly in practice while still maintaining a near-identical worst-case bound on total regret. To speed up the blueprint strategy computation even further, actions with extremely negative regret are not explored in $95 \%$ of iterations.

The blueprint strategy for Pluribus was computed in 8 days on a 64 -core server for a total of $12,400 \mathrm{CPU}$ core hours. It required less than 512 GB of memory. At current cloud computing spot instance rates, this would cost about $\$ 144$ to produce. This is in sharp contrast to all the other recent superhuman AI milestones for games, which used large numbers of servers and/or farms of GPUs. More memory and computation would enable a finer-grained blueprint that would lead to better performance, but would also result in Pluribus using more memory or being slower during real-time search.

We set the size of the blueprint strategy abstraction to allow Pluribus to run during live play on a machine with no more than 128 GB of memory while storing a compressed form of the blueprint strategy in memory.

## Depth-limited search in imperfect-information games

The blueprint strategy for the entire game is necessarily coarse-grained owing to the size and complexity of no-limit Texas hold'em. Pluribus only plays according to this blueprint strategy in the first betting round (of four), where the number of decision points is small enough that the blueprint strategy can afford to not use information abstraction and have a lot of actions in the action abstraction. After the first round (and even in the first round if an opponent chooses a bet size that is sufficiently different from the sizes in the blueprint action abstraction) Pluribus instead conducts real-time search to determine a better, finer-grained strategy for the current situation it is in. For opponent bets on the first round that are slightly off the tree, Pluribus rounds the bet to a nearby ontree size (using the pseudoharmonic mapping (39)) and proceeds to play according to the blueprint as if the opponent had used the latter bet size.

Real-time search has been necessary for achieving superhuman performance in many perfect-information games, including backgammon (18), chess (8), and Go (9, 19). For example, when determining their next move, chess AIs commonly look some number of moves ahead until a leaf node is reached at the depth limit of the algorithm's lookahead. An evaluation function then estimates the value of the board configuration at the leaf node if both players were to play a Nash equilibrium from that point forward. In principle, if an AI could accurately calculate the value of every leaf node (e.g., win, draw, or loss), this algorithm would choose the optimal next move.

However, search as has been done in perfect-information games is fundamentally broken when applied to imperfectinformation games. For example, consider a sequential form of Rock-Paper-Scissors, illustrated in Fig. 3, in which Player 1 acts first but does not reveal her action to Player 2, followed by Player 2 acting. If Player 1 were to conduct search that looks just one move ahead, every one of her actions would appear to lead to a leaf node with zero value. After all, if Player 2 plays the Nash equilibrium strategy of choosing each action with $\frac{1}{3}$ probability, the value to Player 1 of choosing Rock is zero, as is the value of choosing Scissors. So Player 1's search algorithm could choose to always play Rock because, given the values of the leaf nodes, this appears to be equally good as any other strategy.

Indeed, if Player 2's strategy were fixed to always playing the Nash equilibrium, always playing Rock would be an optimal Player 1 strategy. However, in reality Player 2 could
adjust to a strategy of always playing Paper. In that case, the value of always playing Rock would actually be -1 .

This example illustrates that in imperfect-information subgames (the part of the game in which search is being conducted) (40), leaf nodes do not have fixed values. Instead, their values depend on the strategy that the searcher chooses in the subgame (that is, the probabilities that the searcher assigns to his actions in the subgame). In principle, this could be addressed by having the value of a subgame leaf node be a function of the searcher's strategy in the subgame, but this is impractical in large games. One alternative is to make the value of a leaf node conditional only on the belief distribution of both players at that point in the game. This was used to generate the two-player poker AI DeepStack (5). However, this option is extremely expensive because it requires one to solve huge numbers of subgames that are conditional on beliefs. It becomes even more expensive as the amount of hidden information or the number of players grows. The twoplayer poker AI Libratus sidestepped this issue by only doing real-time search when the remaining game was short enough that the depth limit would extend to the end of the game (6). However, as the number of players grows, always solving to the end of the game also becomes computationally prohibitive.

Pluribus instead uses a modified form of an approach that we recently designed-previously only for two-player zerosum games (41)-in which the searcher explicitly considers that any or all players may shift to different strategies beyond the leaf nodes of a subgame. Specifically, rather than assuming all players play according to a single fixed strategy beyond the leaf nodes (which results in the leaf nodes having a single fixed value) we instead assume that each player may choose between $k$ different strategies, specialized to each player, to play for the remainder of the game when a leaf node is reached. In the experiments in this paper, $k=4$. One of the four continuation strategies we use in the experiments is the precomputed blueprint strategy, another is a modified form of the blueprint strategy in which the strategy is biased toward folding, another is the blueprint strategy biased toward calling, and the final option is the blueprint strategy biased toward raising. This technique results in the searcher finding a strategy that is more balanced because choosing an unbalanced strategy (e.g., always playing Rock in Rock-Paper-Scissors) would be punished by an opponent shifting to one of the other continuation strategies (e.g., always playing Paper).

Another major challenge of search in imperfect-information games is that a player's optimal strategy for a particular situation depends on what the player's strategy is for every situation the player could be in from the perspective of her opponents. For example, suppose the player is holding the best possible hand. Betting in this situation could be a good action. But if the player bets in this situation only when
holding the best possible hand, then the opponents would know to always fold in response.

To cope with this, Pluribus keeps track of the probability it would have reached the current situation with each possible hand according to its strategy. Regardless of which hand Pluribus is actually holding, it will first calculate how it would act with every possible hand, being careful to balance its strategy across all the hands so as to remain unpredictable to the opponent. Once this balanced strategy across all hands is computed, Pluribus then executes an action for the hand it is actually holding. The structure of a depth-limited imperfectinformation subgame as used in Pluribus is shown in Fig. 4.

Pluribus used one of two different forms of CFR to compute a strategy in the subgame depending on the size of the subgame and the part of the game. If the subgame is relatively large or it is early in the game, then Monte Carlo Linear CFR is used just as it was for the blueprint strategy computation. Otherwise, Pluribus uses an optimized vector-based form of Linear CFR (38) that samples only chance events (such as board cards) (42).

When playing, Pluribus runs on two Intel Haswell E52695 v3 CPUs and uses less than 128 GB of memory. For comparison, AlphaGo used 1,920 CPUs and 280 GPUs for realtime search in its 2016 matches against top Go professional Lee Sedol (43), Deep Blue used 480 custom-designed chips in its 1997 matches against top chess professional Garry Kasparov (8), and Libratus used 100 CPUs in its 2017 matches against top professionals in two-player poker (6). The amount of time Pluribus takes to conduct search on a single subgame varies between 1 s and 33 s depending on the particular situation. On average, Pluribus plays at a rate of 20 s per hand when playing against copies of itself in six-player poker. This is roughly twice as fast as professional humans tend to play.

## Experimental evaluation

We evaluated Pluribus against elite human professionals in two formats: five human professionals playing with one copy of Pluribus ( $5 \mathrm{H}+1 \mathrm{AI}$ ), and one human professional playing with five copies of Pluribus ( $1 \mathrm{H}+5 \mathrm{AI}$ ). Each human participant has won more than $\$ 1$ million playing poker professionally. Performance was measured using the standard metric in this field of AI, milli big blinds per game (mbb/game). This measures how many big blinds (the initial money the second player must put into the pot) were won on average per thousand hands of poker. In all experiments, we used the vari-ance-reduction technique AIVAT (44) to reduce the luck factor in the game (45) and measured statistical significance at the $95 \%$ confidence level using a one-tailed $t$ test to determine whether Pluribus is profitable.

The human participants in the $5 \mathrm{H}+1 \mathrm{AI}$ experiment were Jimmy Chou, Seth Davies, Michael Gagliano, Anthony Gregg, Dong Kim, Jason Les, Linus Loeliger, Daniel McAulay, Greg

Merson, Nicholas Petrangelo, Sean Ruane, Trevor Savage, and Jacob Toole. In this experiment, 10,000 hands of poker were played over 12 days. Each day, five volunteers from the pool of professionals were selected to participate based on availability. The participants were not told who else was participating in the experiment. Instead, each participant was assigned an alias that remained constant throughout the experiment. The alias of each player in each game was known, so that players could track the tendencies of each player throughout the experiment. $\$ 50,000$ was divided among the human participants based on their performance to incentivize them to play their best. Each player was guaranteed a minimum of $\$ 0.40$ per hand for participating, but this could increase to as much as $\$ 1.60$ per hand based on performance. After applying AIVAT, Pluribus won an average of 48 $\mathrm{mbb} /$ game (with a standard error of $25 \mathrm{mbb} /$ game). This is considered a very high win rate in six-player no-limit Texas hold'em poker, especially against a collection of elite professionals, and implies that Pluribus is stronger than the human opponents. Pluribus was determined to be profitable with a p-value of 0.028 . The performance of Pluribus over the course of the experiment is shown in Fig. 5. Due to the extremely high variance in no-limit poker and the impossibility of applying AIVAT to human players, the win rate of individual human participants could not be determined with statistical significance.

The human participants in the $1 \mathrm{H}+5 \mathrm{AI}$ experiment were Chris "Jesus" Ferguson and Darren Elias. Each of the two humans separately played 5,000 hands of poker against five copies of Pluribus. Pluribus does not adapt its strategy to its opponents and does not know the identity of its opponents, so the copies of Pluribus could not intentionally collude against the human player. To incentivize strong play, we offered each human $\$ 2,000$ for participation and an additional $\$ 2,000$ if he performed better against the AI than the other human player did. The players did not know who the other participant was and were not told how the other human was performing during the experiment. For the 10,000 hands played, Pluribus beat the humans by an average of 32 $\mathrm{mbb} / \mathrm{game}$ (with a standard error of $15 \mathrm{mbb} /$ game). Pluribus was determined to be profitable with a p-value of 0.014. (Darren Elias was behind Pluribus by $40 \mathrm{mbb} / \mathrm{game}$ with a standard error of $22 \mathrm{mbb} /$ game and a p-value of 0.033 , and Chris Ferguson was behind Pluribus by $25 \mathrm{mbb} /$ game with a standard error of $20 \mathrm{mbb} / \mathrm{game}$ and a p-value of 0.107 . Ferguson's lower loss rate may be a consequence of variance, skill, and/or the fact that he used a more conservative strategy that was biased toward folding in unfamiliar difficult situations.)

Because Pluribus's strategy was determined entirely from self-play without any human data, it also provides an outside perspective on what optimal play should look like in multiplayer no-limit Texas hold'em. Pluribus confirms the
conventional human wisdom that limping (calling the "big blind" rather than folding or raising) is suboptimal for any player except the "small blind" player who already has half the big blind in the pot by the rules, and thus has to invest only half as much as the other players to call. While Pluribus initially experimented with limping when computing its blueprint strategy offline through self play, it gradually discarded this action from its strategy as self play continued. However, Pluribus disagrees with the folk wisdom that "donk betting" (starting a round by betting when one ended the previous betting round with a call) is a mistake; Pluribus does this far more often than professional humans do.

## Conclusions

Forms of self play combined with forms of search has led to a number of high-profile successes in perfect-information two-player zero-sum games. However, most real-world strategic interactions involve hidden information and more than two players. This makes the problem very different and significantly more difficult both theoretically and practically. Developing a superhuman AI for multiplayer poker was a widely-recognized milestone in this area and the major remaining milestone in computer poker. In this paper we described Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold'em poker, the most commonly played poker format in the world. Pluribus's success shows that despite the lack of known strong theoretical guarantees on performance in multiplayer games, there are large-scale, complex multiplayer imperfect-information settings in which a carefully constructed self-play-withsearch algorithm can produce superhuman strategies.

## REFERENCES AND NOTES

1. D. Billings, A. Davidson, J. Schaeffer, D. Szafron, The challenge of poker. Artif. Intell. 134, 201-240 (2002). doi:10.1016/S0004-3702(01)00130-8
2. J. von Neumann, Zur Theorie der Gesellschaftsspiele. Math. Ann. 100, 295-320 (1928). doi:10.1007/BF01448847
3. J. Nash, Non-Cooperative Games. Ann. Math. 54, 286 (1951). doi:10.2307/1969529
4. M. Bowling, N. Burch, M. Johanson, O. Tammelin, Computer science. Heads-up limit hold'em poker is solved. Science 347, 145-149 (2015). doi:10.1126/science. 1259433 Medline
5. M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, M. Bowling, DeepStack: Expert-level artificial intelligence in heads-up no-limit poker. Science 356, 508-513 (2017). doi:10.1126/science.aam6960 Medline
6. N. Brown, T. Sandholm, Superhuman Al for heads-up no-limit poker: Libratus beats top professionals. Science 359, 418-424 (2018). doi:10.1126/science.aao1733 Medline
7. J. Schaeffer, One Jump Ahead: Challenging Human Supremacy in Checkers (Springer-Verlag, New York, 1997).
8. M. Campbell, A. J. Hoane Jr., F.-H. Hsu, Deep Blue. Artif. Intell. 134, 57-83 (2002). doi:10.1016/S0004-3702(01)00129-1
9. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis, Mastering the game of Go with deep neural networks and tree search. Nature 529, 484-489 (2016). doi:10.1038/nature16961 Medline
10. Recently, in the real-time strategy games Dota 2 (20) and StarCraft 2 (21), Als have beaten top humans, but as humans have gained more experience against the Als, humans have learned to beat them. This may be because for those two-player zero-sum games, the Als were generated by techniques not guaranteed to converge to a Nash equilibrium, so they do not have the unbeatability property that Nash equilibruim strategies have in two-player zero-sum games. (Dota 2 involves two teams of five players each. However, because the players on the same team have the same objective and are not limited in their communication, the game is two-player zero-sum from an Al and game-theoretic perspective).
11. S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) (2011), pp. 533-540.
12. S. Ganzfried, T. Sandholm, ACM Trans. Econ. Comp. (TEAC) 3, 8 (2015). Best of EC-12 special issue.
13. C. Daskalakis, P. W. Goldberg, C. H. Papadimitriou, The Complexity of Computing a Nash Equilibrium. SIAM J. Comput. 39, 195-259 (2009). doi:10.1137/070699652
14. X. Chen, X. Deng, S.-H. Teng, Settling the complexity of computing two-player Nash equilibria. J. Assoc. Comput. Mach. 56, 14 (2009). doi:10.1145/1516512.1516516
15. A. Rubinstein, Inapproximability of Nash Equilibrium. SIAM J. Comput. 47, 917-959 (2018). doi:10.1137/15M1039274
16. K. Berg, T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2017).
17. M. A. Zinkevich, M. Bowling, M. Wunder, The lemonade stand game competition: Solving unsolvable puzzles. ACM SIGecom Exchanges 10, 35-38 (2011). doi:10.1145/1978721.1978730
18. G. Tesauro, Temporal difference learning and TD-Gammon. Commun. ACM 38, 58-68 (1995). doi:10.1145/203330.203343
19. D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, D. Hassabis, Mastering the game of Go without human knowledge. Nature 550, 354-359 (2017). doi:10.1038/nature24270 Medline
20. A. I. Open, A. I. Open, Five, https://blog.openai.com/openai-five/ (2018).
21. O. Vinyals et al., AlphaStar: Mastering the Real-Time Strategy Game StarCraft II, https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/ (2019).
22. L. S. Shapley, Advances in Game Theory, M. Drescher, L. S. Shapley, A. W. Tucker, Eds. (Princeton Univ. Press, 1964).
23. R. Gibson, Regret minimization in games and the development of champion multiplayer computer poker-playing agents, Ph.D. thesis, University of Alberta (2014).
24. T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2015), pp. 41274131. Senior Member Track.
25. T. Sandholm, Computer science. Solving imperfect-information games. Science 347, 122-123 (2015). doi:10.1126/science.aaa4614 Medline
26. M. Johanson, N. Burch, R. Valenzano, M. Bowling, in International Conference on Autonomous Agents and Multiagent Systems (AAMAS) (2013), pp. 271-278.
27. S. Ganzfried, T. Sandholm, in AAAI Conference on Artificial Intelligence (AAAI) (2014), pp. 682-690.
28. N. Brown, S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and Multiagent Systems (AAMAS) (2015), pp. 7-15.
29. M. Zinkevich, M. Johanson, M. H. Bowling, C. Piccione, in Neural Information Processing Systems (NeurIPS) (2007), pp. 1729-1736.
30. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information (2013).
31. M. B. Johanson, Robust strategies and counter-strategies: from superhuman to optimal play, Ph.D. thesis, University of Alberta (2016).
32. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information (2016).
33. N. Brown, T. Sandholm, in International Joint Conference on Artificial Intelligence (IJCAI) (2016), pp. 4238-4239.
34. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information Games (2017).
35. M. Lanctot, K. Waugh, M. Zinkevich, in M. Bowling, Neural Information Processing Systems (NeurIPS) (2009), pp. 1078-1086.
36. M. Johanson, N. Bard, M. Lanctot, R. Gibson, M. Bowling, in International
37. R. Gibson, M. Lanctot, N. Burch, D. Szafron, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2012), pp. 1355-1361.
38. N. Brown, T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2019).
39. S. Ganzfried, T. Sandholm, in International Joint Conference on Artificial Intelligence (IJCAI) (2013), pp. 120-128.
40. Here we use term subgame the way it is usually used in AI. In game theory that word is used differently by requiring a subgame to start with a node where the player whose turn it is to move has no uncertainty about state-in particular, no uncertainty about the opponents' private information.
41. N. Brown, T. Sandholm, B. Amos, in Neural Information Processing Systems (NeurIPS) (2018), pp. 7663-7674.
42. M. Johanson, K. Waugh, M. Bowling, M. Zinkevich, in International Joint Conference on Artificial Intelligence (IJCAI) (2011), pp. 258-265.
43. E. P. DeBenedictis, Rebooting Computers as Learning Machines. Computer 49, 84-87 (2016). doi:10.1109/MC.2016.156
44. N. Burch, M. Schmid, M. Moravcik, D. Morill, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2018), pp. 949-956.
45. Due to the presence of AIVAT and because the players did not know each others' scores during the experiment, there was no incentive for the players to play a riskaverse or risk-seeking strategy in order to outperform the other human.
46. A. Gilpin, T. Sandholm, Lossless abstraction of imperfect information games. J. Assoc. Comput. Mach. 54, 25 (2007). doi:10.1145/1284320.1284324
47. K. Waugh, AAAI Workshop on Computer Poker and Imperfect Information (2013).
48. A. Gilpin, T. Sandholm, T. B. Sørensen, in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) (2007), pp. 50-57.
49. S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS) (2015), pp. 37-45.
50. N. Burch, M. Johanson, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2014), pp. 602-608.
51. M. Moravcik, M. Schmid, K. Ha, M. Hladik, S. Gaukrodger, in AAAI Conference on Artificial Intelligence (AAAI) (2016), pp. 572-578.
52. N. Brown, T. Sandholm, in Neural Information Processing Systems (NeurIPS) (2017), pp. 689-699.

## ACKNOWLEDGMENTS

We thank Pratik Ringshia for building a GUI and thank Jai Chintagunta, Ben Clayman, Alex Du, Carl Gao, Sam Gross, Thomas Liao, Christian Kroer, Joe Langas, Adam Lerer, Vivek Raj, and Steve Wu for playing against Pluribus as early testing. Funding: This material is based on Carnegie Mellon University research supported by the National Science Foundation under grants IIS-1718457, IIS1617590, IIS-1901403, and CCF-1733556, and the ARO under award W911NF-17-1-0082, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center. Facebook funded the player payments. Author contributions: N.B. and T.S. designed the algorithms. N.B. wrote the code. N.B. and T.S. designed the experiments and wrote the paper. Competing interests: The authors have ownership interest in Strategic Machine, Inc. and Strategy Robot, Inc. which have exclusively licensed prior game-solving code from Prof. Sandholm's Carnegie Mellon University laboratory, which constitutes the bulk of the code in Pluribus. Data and materials availability: The data presented in this paper are shown in the main text and supplementary materials. Because poker is played commercially, the risk associated with releasing the code outweighs the benefits. To aid reproducibility, we have included the pseudocode for the major components of our program in the supplementary materials.

## SUPPLEMENTARY MATERIALS

science.sciencemag.org/cgi/content/full/science.aay2400/DC1
Supplementary Text
Table S1
References (46-52)
Data File S1
31 May 2019; accepted 2 July 2019
Published online 11 July 2019
![](https://cdn.mathpix.com/cropped/2025_03_19_935ee71d7e2175c4e6c5g-08.jpg?height=603&width=1218&top_left_y=235&top_left_x=451)

Fig. 1. An example of the equilibrium selection problem. In the Lemonade Stand Game, players simultaneously choose a point on a ring and want to be as far away as possible from any other player. In every Nash equilibrium, players are spaced uniformly around the ring. There are infinitely many such Nash equilibria. However, if each player independently chooses one Nash equilibrium to play, their joint strategy is unlikely to be a Nash equilibrium. Left: An illustration of three different Nash equilibria in this game, distinguished by three different colors. Right: Each player independently chooses one Nash equilibrium. Their joint strategy is not a Nash equilibrium.
![](https://cdn.mathpix.com/cropped/2025_03_19_935ee71d7e2175c4e6c5g-09.jpg?height=632&width=1791&top_left_y=237&top_left_x=154)

Fig. 2. A game tree traversal via Monte Carlo CFR. In this figure player $P_{1}$ is traversing the game tree. Left: A game is simulated until an outcome is reached. Middle: For each $P_{1}$ decision point encountered in the simulation in the Left figure, $P_{1}$ explores each other action that $P_{1}$ could have taken and plays out a simulation to the end of the game. $P_{1}$ then updates its strategy to pick actions with higher payoff with higher probability. Right: $P_{1}$ explores each other action that $P_{1}$ could have taken at every new decision point encountered in the Middle figure, and $P_{1}$ updates its strategy at those hypothetical decision points. This process repeats until no new $P_{1}$ decision points are encountered, which in this case is after three steps but in general may be more. Our implementation of MCCFR (described in the supplementary material) is equivalent but traverses the game tree in a depth-first manner. (The percentages in the figure are for illustration purposes only and may not correspond to actual percentages that the algorithm would compute.)
![](https://cdn.mathpix.com/cropped/2025_03_19_935ee71d7e2175c4e6c5g-10.jpg?height=1009&width=1167&top_left_y=233&top_left_x=471)

Fig. 3. Perfect-information game search in Rock-Paper-Scissors. Top: A sequential representation of Rock-Paper-Scissors in which Player 1 acts first but does not reveal her action to Player 2, who acts second. The dashed lines between the Player 2 nodes signify that Player 2 does not know which of those nodes he is in. The terminal values are shown only for Player 1. Bottom: A depiction of the depth-limited subgame if Player 1 conducts search (with a depth of one) using the same approach as is used in perfect-information games. The approach assumes that after each action Player 2 will play according to the Nash equilibrium strategy of choosing Rock, Paper, and Scissors with $\frac{1}{3}$ probability each. This results in a value of zero for Player 1 regardless of her strategy.
![](https://cdn.mathpix.com/cropped/2025_03_19_935ee71d7e2175c4e6c5g-11.jpg?height=608&width=1827&top_left_y=238&top_left_x=149)

Fig. 4. Real-time search in Pluribus. The subgame shows just two players for simplicity. A dashed line between nodes indicates that the player to act does not know which of the two nodes she is in. Left: The original imperfectinformation subgame. Right: The transformed subgame that is searched in real time to determine a player's strategy. An initial chance node reaches each root node according to the normalized probability that the node is reached in the previously-computed strategy profile (or according to the blueprint strategy profile if this is the first time in the hand that real-time search is conducted). The leaf nodes are replaced by a sequence of new nodes in which each player still in the hand chooses among $k$ actions, with no player first observing what another player chooses. For simplicity, $k=2$ in the figure. In Pluribus, $k=4$. Each action in that sequence corresponds to a selection of a continuation strategy for that player for the remainder of the game. This effectively leads to a terminal node (whose value is estimated by rolling out the remainder of the game according to the list of continuation strategies the players chose).
![](https://cdn.mathpix.com/cropped/2025_03_19_935ee71d7e2175c4e6c5g-12.jpg?height=1253&width=1188&top_left_y=252&top_left_x=466)

Fig. 5. Performance of Pluribus in the 5 humans +1 AI experiment. Top: The lines show the win rate (solid line) plus or minus the standard error (dashed lines). Bottom: The lines show the cumulative number of chips won (solid line) plus or minus the standard error (dashed lines). The relatively steady performance of Pluribus over the course of the 10,000hand experiment suggests the humans were unable to find exploitable weaknesses in the bot.



# Supplementary Materials for Superhuman AI for multiplayer poker 

Noam Brown* and Tuomas Sandholm*<br>*Corresponding author. E-mail: noamb@cs.cmu.edu (N.B.); sandholm@cs.cmu.edu (T.S.)<br>Published 11 July 2019 on Science First Release

DOI: 10.1126/science.aay2400
This PDF file includes:
Supplementary Text
Table S1
References

Other Supplementary Materials for this manuscript include the following: (available at science.sciencemag.org/cgi/content/full/science.aay2400/DC1)

Data File S1 as zipped archive

## Summary of technical innovations in Pluribus relative to prior poker AI systems

In this section we briefly summarize the technical innovations in Pluribus relative to prior poker AI systems. These techniques are discussed in more detail in the body of the paper or later in this supplementary material.

We split the innovations into three categories in the following three subsections, respectively. Unfortunately, measuring the impact of each of these innovations would be too expensive due to the extremely high variance in no-limit poker and the high dollar and time cost of conducting experiments against humans. We provide estimates of the magnitude of the improvement where possible.

## Depth-limited search

Depth-limited search was the most important improvement that made six-player poker possible. It reduces the computational resources and memory needed by probably at least five orders of magnitude. Libratus (6) always solved to the end of the game when real-time search was used (Libratus started using real-time search on the third betting round-the half-way point in the game-and in certain situations even earlier). However, additional players increase the size of subgames exponentially. Conducting beneficial real-time search on the flop (the second betting round) with more than three players involved is likely infeasible without depth-limited search. DeepStack (5) used a form of depth-limited search in two-player poker, but the technique was already very expensive computationally in that context-because huge numbers of belief-contingent subgames needed to be solved in advance to obtain leaf values for the realtime search; this may not be feasible in six-player poker. We previously published a two-player
version of depth-limited search in imperfect-information games that is far less expensive in two-player poker than the real-time solving in Libratus or DeepStack (41). In this paper we generalized that approach to more than two players and included a number of additional technical contributions as follows.

- Our previous depth-limited search had the searcher play the blueprint strategy while the opponent chose among multiple continuation strategies. This is theoretically sound (in two-player zero-sum games), but in practice gives the opponents more power and therefore makes the searcher relatively defensive/conservative. The current paper addresses this weakness by having the searcher also choose among continuous strategies (which is still theoretically sound in two-player zero-sum games). Our prior search algorithm instead penalized the opponent to try to balance the players, but our new approach is more effective, easier, and more elegant.
- Previous nested search algorithms either used nested safe search or nested unsafe search (described in detail later in this supplementary material). Nested unsafe search is not theoretically sound, and in some cases may do extremely poorly, but on average tends to perform better in practice. We use a new form of nested unsafe search in this paper in which we always solve starting at the beginning of the current betting round rather than starting at the most recent decision point. Our player's strategy is held fixed for actions that it has already chosen in the betting round. However, the approach allows the other players to have changed strategies anywhere in the current betting round, that is, even above the current decision point. Taking that possibility into account mitigates the potential for unsafe search to be exploitable, while still retaining its practical benefits.


## Equilibrium finding

Pluribus also has innovations in the equilibrium finding algorithms that are used in the blueprint computation and within the depth-limited search. Here we summarize them and we describe each of them in detail in the paper body or later in this supplementary material.

- We used Linear MCCFR rather than traditional MCCFR. While we already published Linear MCCFR recently (38), it was never implemented and tested at scale. We suspect this sped up convergence by about a factor of 3 .
- Our Linear MCCFR algorithm used a form of dynamic pruning that skipped actions with extremely negative regret in $95 \%$ of iterations. A similar idea was used in Libratus and in BabyTartanian8 (our AI that won the 2016 Annual Computer Poker Competition), but in those cases the skipping was done everywhere. In contrast, in Pluribus, in order to reduce the potential inaccuracies involved with pruning, we do not skip actions on the last betting round (because on the last betting round one does not get the benefits of effectively increasing the card abstraction through pruning) or actions leading to terminal payoffs (because the cost of examining those payoffs is minor anyway). Additionally, whether or not to skip in Libratus/BabyTartanian8 was decided separately for each action rather than deciding for the entire iteration; the latter is cheaper due to fewer calls to a random number generator, so we do the latter in Pluribus. We suspect that these changes contributed about a factor of 2 speedup.


## Memory usage

To conserve memory, Pluribus allocates memory for the regrets in an action sequence only when the sequence is encountered for the first time (except on the first betting round which is small and allocated up front). This is particularly useful in 6-player poker, in which there are many
action sequences that never occur. This reduced memory usage by more than a factor of 2 .

## Rules for no-limit Texas hold'em poker

No-limit Texas hold'em (NLTH) has been the most common form of poker for more than a decade. It is used, for example, to determine the winner of the World Series of Poker Main Event. Six-player NLTH poker is the most widely played format of NLTH.

Each player starts each hand with $\$ 10,000$ and the blinds (the amount of money that Player 1 and Player 2 must contribute to the pot before play begins) are $\$ 50$ and $\$ 100$, so each player starts with 100 big blinds, which is the standard buy-in amount for both live and online play. By having each hand start with the same number of chips, we are able to treat each hand as a separate sample when measuring win rate.

NLTH consists of four rounds of betting. On a round of betting, each player can choose to either fold, call, or raise. If a player folds, they are considered to no longer be part of the hand. That is, the player cannot win any money in the pot and takes no further actions. If a player calls, that players places a number of chips in the pot equal to the most that any other player has contributed to the pot. If a player raises, that player adds more chips to the pot than any other player so far. A round ends when each player still in the hand has acted and has the same amount of money in the pot as every other player still in the hand.

The initial raise on each round must be at least $\$ 100$. Any subsequent raise on the round must be at least as large as the previous raise (i.e., at least as large as the amount of money beyond a call that the previously-raising player contributed). No player can raise more than their remaining amount of money (which starts at $\$ 10,000$ ).

The first round starts with Player 3 and each subsequent round starts with Player 1 (if Player 1 is still in the hand). Play proceeds through each consecutive player still in the hand, with Player 1 following Player 6 if the round has not yet ended. On the first round, Player 1
must contribute $\$ 50$ and Player 2 must contribute $\$ 100$ before play begins.
At the start of the first round, every player receive two private cards from a standard 52-card deck. At the start of the second round, three community cards are dealt face up for all players to observe. At the start of the third betting round, an additional community card is dealt face up. At the start of the fourth betting round, a final fifth community card is dealt face up. If at any point only one player remains in the hand, that player collects all the money that all players have contributed to the pot. Otherwise, the player with the best five-card poker hand, constructed from the player's two private cards and the five face-up community cards, wins the pot. In the case of a tie, the pot is split equally among the winning players.

For the next hand-i.e., the next repetition of the game-player 2 becomes player 1, player 3 becomes player 2, etc.

## Experimental setup and participants

The players were allowed to take as long as they wanted for any decision. The players were instructed to not use any software to assist them with forming their strategy while playing.

The two human participants in the $1 \mathrm{H}+5 \mathrm{AI}$ experiment were Chris "Jesus" Ferguson and Darren Elias. Chris Ferguson has won more than $\$ 9.2$ million playing live poker and has won six World Series of Poker events, including most famously the 2000 World Series of Poker Main Event. Darren Elias has won more than $\$ 7.1$ million playing live poker and $\$ 3.8$ million playing online poker. He holds four World Poker Tour titles and two World Championship of Online Poker titles. He is regarded as an elite player in this specific form of poker.

Both players were allowed to play at home on their own schedule. They could take as much time for any decision as they wanted and were told Pluribus would not change its strategy based on how long they took to make a decision. They were allowed-but not required-to play up to four tables simultaneously. Elias usually chose to play four tables while Ferguson usually
chose to play one.
The human participants in the $5 \mathrm{H}+1 \mathrm{AI}$ experiment were Jimmy Chou, Seth Davies, Michael Gagliano, Anthony Gregg, Dong Kim, Jason Les, Linus Loeliger, Daniel McAulay, Greg Merson, Nicholas Petrangelo, Sean Ruane, Trevor Savage, and Jacob Toole. All participants have won at least $\$ 1$ million playing poker professionally.

Each human player was assigned an alias that they would use throughout the experiment. The humans could see the aliases of the humans they were playing with, but did not know the identity of the human behind the alias. We made no effort to mask the identity of Pluribus. On each day, five players were selected among those who volunteered to play from the pool of 13 players. In some cases due to player availability, the day was divided into multiple sessions with a different set of players in each session. The length of play on each day lasted between three hours and eight hours, but was typically four hours. There were breaks every two hours lasting at least ten minutes. The assignment of players to seats (i.e., player order) was determined randomly at the beginning of each day.

Players were asked, but not required, to play four tables simultaneously. This was later changed to a maximum of six tables at the request of the players (due to the lack of a time limit on decisions). The players played at a pace of about 180 hands per hour when playing four tables. The humans were asked to not try to uncover the identity of any other participant in the experiment, to not mention the experiment to anyone while it was happening, and to not intentionally collude with any player (which is not allowed in poker).

The players were paid a minimum of $\$ 0.40$ per hand played, but this could increase to as much as $\$ 1.60$ per hand based on performance. The compensation structure was designed to as closely as possible align the incentives of the players with those of a normal cash game, while still guaranteeing the players would not lose money and would divide a fixed-sized prize pool. Specifically, each player was paid $\$(1+0.005 X)$ per hand, where $X$ is the player's adjusted
variance-reduced win rate in terms of mbb/game with a floor at $X=-120$ and a ceiling at $X=120$. Any player that lost more than $-120 \mathrm{mbb} /$ game or won more than $120 \mathrm{mbb} /$ game after the application of variance reduction had the excess winnings or losses divided among all the players proportional to the number of hands each player played (after first canceling out all players' excess winnings with all players' excess losses as much as possible and canceling out all of Pluribus's winnings or losses). Players were not informed of their variance-reduced win rates until after the experiment was completed.

## Statistical analysis

For the $1 \mathrm{H}+5 \mathrm{AI}$ experiments, each participant played 5,000 hands for a total of 10,000 hands. For the $5 \mathrm{H}+1 \mathrm{AI}$ experiment, a total of 10,000 hands were played. In both the $5 \mathrm{H}+1 \mathrm{AI}$ experiment and the $1 \mathrm{H}+5 \mathrm{AI}$ experiment, we used a one-tailed test to determine at the $95 \%$ confidence level whether Pluribus was stronger than the humans.

Each hand in each experiment format was treated as an independent and identically distributed sample. (Theoretically speaking, this assumption may not be exactly accurate because humans may adjust their strategy over the course of the experiment. Nevertheless, this assumption is the standard assumption that is used in this field of research.)

In the case of the $1 \mathrm{H}+5 \mathrm{AI}$ experiment, we only measure whether Pluribus is profitable against both players in aggregate since that was what the experiment was designed to measure. In fact, the only reason we used two humans is that it would have taken prohibitively long for one human to complete all of the 10,000 hands. That said, we also do report p-values for the individual players for completeness. Chris Ferguson was behind by an average of 25.2 $\mathrm{mbb} / \mathrm{game}$ with a standard error of $20.2 \mathrm{mbb} / \mathrm{game}$, which would be a p-value of 0.106 . Darren Elias was behind by an average of $40.2 \mathrm{mbb} / \mathrm{game}$ with a standard error of 21.9 , which would be a p-value of 0.033 . Across both players, the average was $32.7 \mathrm{mbb} / \mathrm{game}$ with a standard error
of $14.9 \mathrm{mbb} / \mathrm{game}$. Pluribus was determined to be profitable with a p-value of 0.014 , which is statistically significant at the $95 \%$ confidence level, the standard threshold used in this field of research.

For the $5 \mathrm{H}+1 \mathrm{AI}$ experiment, Pluribus earned an average of $47.7 \mathrm{mbb} / \mathrm{game}$ with a standard error of $25.0 \mathrm{mbb} / \mathrm{game}$. Pluribus was determined to be profitable with a p-value of 0.028 , which is statistically significant at the $95 \%$ confidence level.

In the $5 \mathrm{H}+1 \mathrm{AI}$ experiment the human participants played an additional 13 hands beyond the requested 10,000 . We did not include those additional hands in our analysis. With the additional hands included, Pluribus's win rate would increase to $50.9 \mathrm{mbb} / \mathrm{game}$.

AIVAT could not be meaningfully applied for individual human players because AIVAT can only account for chance nodes, Pluribus's strategy across all possible hands, and Pluribus's probability distribution over actions. For human players, their probability distribution over actions and their strategy across all possible hands is unknown. Since most hands a human plays only minimally involves Pluribus, that only leaves chance nodes as something AIVAT can always control for. Table $\$ 1$ shows results for each human participant (listing only their alias) after applying a relatively modest variance-reduction technique that for each player subtracts the expected value (according to Pluribus's blueprint) of each hand given all players' private cards.

## Notation and background

In an imperfect-information extensive-form (i.e., tree-form) game there is a finite set of players, $\mathcal{P}$. A node (i.e., history) $h$ is defined by all information of the current situation, including private knowledge known to only one player. $A(h)$ denotes the actions available at a node and $P(h)$ is either chance or the player whose turn it is to act at that node. We write $h \prec h^{\prime}$ if a sequence of actions leads from $h$ to $h^{\prime}$. We represent the node that follows $h$ after choosing action $a$ by
$h \cdot a . H$ is the set of all nodes. $Z \subseteq H$ are terminal nodes for which no actions are available and which award a value to each player.

Imperfect information is represented by information sets (infosets) for each player $p \in \mathcal{P}$. For any infoset $I$ belonging to $p$, all nodes $h, h^{\prime} \in I$ are indistinguishable to $p$. Moreover, every non-terminal node $h \in H$ belongs to exactly one infoset for each $p$.

A strategy (i.e., a policy) $\sigma(I)$ is a probability vector over actions for acting player $p$ in infoset $I$. Since all states in an infoset belonging to $p$ are indistinguishable, the strategies in each of them must be identical. The set of actions in $I$ is denoted by $A(I)$. The probability of a particular action $a$ is denoted by $\sigma(I, a)$ or by $\sigma(h, a)$. We define $\sigma_{p}$ to be a strategy for $p$ in every infoset in the game where $p$ acts. A strategy profile $\sigma$ is a tuple of strategies, one for each player.

We denote reach by $\pi^{\sigma}(h)$. This is the probability with which $h$ is reached if all players play according to $\sigma . \pi_{p}^{\sigma}(h)$ is the contribution of $p$ to this probability. $\pi_{-p}^{\sigma}(h)$ is the contribution of chance and all players other than $p$.

A public state $G$ is defined as a set of nodes that an outside observer with no access to hidden information cannot distinguish. Formally, for any node $h \in G$, if $h, h^{\prime} \in I$ for some infoset $I$, then $h^{\prime} \in G$. An imperfect-information subgame, which we simply call a subgame, $\mathcal{S}$ is a union of public states where if any node A leads to any node B and both A and B are in $\mathcal{S}$, then every node between A and B is also in $\mathcal{S}$. Formally, for any node $h \in \mathcal{S}$, if $h, h^{\prime} \in I$ for some infoset $I$ then $h^{\prime} \in \mathcal{S}$. Moreover, if $h \in \mathcal{S}$ and $h^{\prime \prime} \in \mathcal{S}$ and there is a sequence of actions leading from $h$ to $h^{\prime \prime}$ (i.e., $h \prec h^{\prime \prime}$ ), then for every node $h^{\prime}$ such that $h \prec h^{\prime} \prec h^{\prime \prime}, h^{\prime} \in \mathcal{S}$.

## Variance reduction via AIVAT

We scored the players using a form of AIVAT modified to handle games with more than two players (44). AIVAT provides an unbiased estimate of the true win rate while reducing variance
in practice by about a factor of 9 . We now provide a brief description of AIVAT.
Every time Pluribus acts at node $h$, its strategy defines a probability distribution over actions. Let $\sigma(h, a)$ be the probability that Pluribus chooses action $a$. Given all players' strategies, there is some unknown value $v(h, a)$ for choosing action $a$. Define $v(h)=\sum_{a \in A(h)} \sigma(h, a) v(h, a)$. $v(h)$ gets passed up to the parent node, which repeats until eventually we can compute $v(\emptyset)$ (the value for the root node, that is, the value for the entire hand). If we knew $v(h, a)$ exactly for each action $a$, then we could simply return $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$. Unfortunately we do not know $v(h, a)$ because that would require knowledge of the other players' strategies. Instead, by choosing action $a$ and playing the rest of the hand, Pluribus eventually receives some value $\hat{v}(h, a)$ that is a sample drawn from a distribution whose expectation is $v(h, a)$.

If we received one sample for each action, then we could return $\sum_{a \in A(h)} \sigma(h, a) \hat{v}(h, a)$ and in expectation this would be the same as $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$. However, since only one action can be chosen during a hand, we define $\hat{v}^{\prime}(h, a)=\hat{v}(h, a)$ if $a$ was the one action that was sampled, and $\hat{v}^{\prime}(h, a)=0$ otherwise. Then $\sum_{a \in A(h)} \hat{v}^{\prime}(h, a)$ is an unbiased estimate of $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$ that only requires a sample for one action (though the variance may be quite high).

While we do not know $v(h, a)$ exactly, we may have a reasonable estimate of it. We can leverage this estimate to compute a lower-variance estimate of $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$ that is still unbiased. Specifically, suppose we have arbitrary values $\tilde{v}(h, a)$ for each action $a$ (where we would like $\tilde{v}(h, a) \approx v(h, a)$, but this is not required). Then $\sum_{a \in A(h)} \sigma(h, a) v(h, a)=$ $\sum_{a \in A(h)} \sigma(h, a)(v(h, a)-\tilde{v}(h, a))+\sum_{a \in A(h)} \sigma(h, a) \tilde{v}(h, a)$. Let $a^{*}$ be the action that is randomly sampled according to probability distribution $\sigma(h)$. Then $\tilde{v}\left(h, a^{*}\right)$ is an unbiased estimate of $\sum_{a \in A(h)} \sigma(h, a) \tilde{v}(h, a)$ and, as previously established, $\hat{v}\left(h, a^{*}\right)$ is an unbiased estimate of $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$. So $\hat{v}\left(h, a^{*}\right)-\tilde{v}\left(h, a^{*}\right)+\sum_{a \in A(h)} \sigma(h, a) \tilde{v}(h, a)$ is an unbiased estimate of $\sum_{a \in A(h)} \sigma(h, a) v(h, a)$. But if $\tilde{v}\left(h, a^{*}\right) \approx v\left(h, a^{*}\right)$, then this estimate will likely
have lower variance than $\hat{v}\left(h, a^{*}\right)$. Moreover, we can pass up the value $\hat{v}\left(h, a^{*}\right)-\tilde{v}\left(h, a^{*}\right)+$ $\sum_{a \in A(h)} \sigma(h, a) \tilde{v}(h, a)$ rather than passing up $\hat{v}\left(h, a^{*}\right)$.

This variance-reduction approach also applies to chance nodes, because the probability distribution at chance nodes is known and fixed. However, it does not apply to human decision points, because the humans do not give a probability distribution for their actions.

Furthermore, Pluribus determines its strategy for every hand it could be holding (its entire range) rather than just the hand it was dealt. It can therefore estimate what its payoff would have been for each of these hands. This lowers variance even further.

Each copy of Pluribus runs AIVAT independently. The human's average win rate is simply the negative of the average of each AI's win rate. To reduce variance even further in the $1 \mathrm{H}+5 \mathrm{AI}$ experiments, we replay each hand with a copy of Pluribus in the human's position. We will refer to this copy of Pluribus in the human's position as the Control. The human's win rate is subtracted by the Control's win rate (which in expectation must be zero, since it is the same agent as its opponents). Moreover, if a hand ends with both the human and the Control folding in the first betting round, and the sequence of actions in the hand up to that point is identical in both cases, then that hand is treated as having zero expected value. This does not bias the result because the human and the Control do not make any further decisions in the hand, and have acted identically up to that point.

## Hardware usage

Pluribus was trained on the Bridges supercomputer at the Pittsburgh Supercomputing Center. The blueprint strategy was trained on a single 64-core large shared memory node that has four 16-core Intel Xeon E5-8860 v3 CPUs. Although the node has 3 TB of memory available, less than 0.5 TB was needed. When playing in real time, Pluribus runs on a single 28-core, 128 GB regular shared memory node. The node uses two 14-core Intel Haswell E5-2695 v3 CPUs. No

GPUs were used at any point.

## Further details of the abstraction algorithm

NLTH is a massive game that would be infeasible to store in memory. Abstraction reduces the size of NLTH by bucketing similar decision points together and by eliminating some actions from the game. A strategy is computed for this simplified, abstract version of the game and that strategy is used as a guide for how the full version of NLTH should be played. We used abstraction in Pluribus both for the precomputed blueprint strategy and for the strategies computed during real-time search.

There are two main forms of abstraction: action abstraction and information abstraction.
Two infosets are bucketed together and treated identically if they share the same action-abstraction sequence and the same information-abstraction bucket.

Action abstraction involves reducing the number of actions that can be taken in the game. NLTH normally allows a player to raise any dollar increment between the minimum legal raise (which may be as low as $\$ 100$ ) and the number of remaining chips (which may be as high as $\$ 10,000$ ). The action abstraction we use allows between one and 14 different raise sizes depending on the particular decision point. All raise sizes are fractions of the size of the pot. These candidate raise sizes for Pluribus's algorithms to consider were decided by hand based on what raise sizes earlier versions of Pluribus decided to use with significant positive probabilities.

The action abstraction of the blueprint strategy is particularly fine-grained on the first betting round because Pluribus usually does not use real-time search on this round. The action abstraction is more coarse on the second betting round. On the third and fourth betting round, there are at most three raise sizes for the first raise in the round (either $0.5 \times$ the size of the pot, $1 \times$ the size of the pot, or all remaining chips), and at most two for the remaining raises in a round ( $1 \times$ the size of the pot or all remaining chips). During real-time search, the number of
raise sizes varies between one and six. The fold and call actions were always included when they were legal actions in both the blueprint and real-time search.

There are a total of $664,845,654$ action sequences in the blueprint action abstraction. However, only $413,507,309$ of them were ever encountered during training. To reduce memory usage, memory for the regrets of an action sequence was only allocated when the action sequence was encountered for the first time. During real-time search, the number of action sequences (considering only players' actions, not chance's actions) in the subgame action abstraction is between 100 and 2,000.

Information abstraction buckets together situations based on the revealed chance outcomes. In poker, this is the private cards dealt to the player and the public board cards. We refer to a sequence of revealed private cards and revealed board cards as an information situation. Lossless abstraction only buckets together strategically identical information situations (46, 47). For example, $2 \mathbf{6}$ is strategically identical to $206 \bigcirc$ on the first betting round, so there is no loss in strategy quality if these two hands are treated identically. If lossless abstraction were applied on each round, there would be $169,1,286,792,55,190,538$, and $2,428,287,420$ strategically unique information situations on the first, second, third, and fourth betting rounds, respectively. However, we only use lossless abstraction on the first betting round. Lossy abstraction buckets together information situations that are not strategically identical, but that ideally are strategically similar. We use lossy abstraction on each betting round after the first one, with 200 buckets per round. That is, on each round after the first one, each information situation is put into one of 200 buckets, with the information situations in each bucket being treated identically. Information situations were bucketed using k-means clustering on domain-specific features (26).

The real-time search algorithm only uses lossless information abstraction for the round it is in. For later rounds, it uses lossy information abstraction with 500 buckets per round. That is, on each round, each information situation is put into one of 500 buckets, with information
situations in the same bucket being treated identically. These buckets were determined separately for each "flop" (the first three revealed board cards) using an algorithm that considers the future potential of each poker hand (28). That algorithm combines the idea of potential-aware abstraction (48) with clustering based on earth-mover distance (27).

## Further details of the blueprint computation algorithm

We computed the blueprint strategy for Pluribus using external-sampling Monte Carlo Counterfactual Regret Minimization $(29,35)$ with two important improvements.

First, we used linear weighting for both the regret and average strategies for the first 400 minutes of the run (38). (We stop the discounting after that because the time cost of doing the multiplications with the discount factor is not worth the benefit later on.) Specifically, after every 10 minutes for the first 400 minutes, the regrets and average strategies were discounted by $\frac{T / 10}{T / 10+1}$, where $T$ is the number of minute that have elapsed since the start of training. Experiments in two-player NLTH subgames show that this modification speeds up convergence by about a factor of 3 (38).

Second, our MCCFR algorithm did not explore every traverser action on every iteration after the first 200 minutes. Instead, for $95 \%$ of iterations, traverser actions with regret below $-300,000,000$ were not explored unless those actions were on the final betting round or those actions immediately led to a terminal node. In the remaining $5 \%$ of iterations, every traverser action was explored. This "pruning" of negative-regret actions means iterations can be completed more quickly. More importantly, it also effectively leads to a finer-grained information abstraction. For example, on the second betting round there are on average 6,434 infosets per abstract infoset bucket. The strategy for that abstract infoset bucket must generalize across all of those 6,434 infosets. But with pruning, many of those 6,434 infosets are traversed only $5 \%$ as often, so the strategy for the abstract infoset can better focus on generalizing across infosets that
are likely to actually be encountered during strong play. Negative-regret pruning has previously been used with great success in two-player NLTH (6, 33, 34). In six-player NLTH, the benefit of this pruning is even larger in practice because good strategies in six-player NLTH involve folding most hands early in the game, so an even smaller fraction of the game tree is reached with positive probability by the final strategy. The pseudocode for the blueprint computation is shown in Algorithm 1 .

To save memory, regrets were stored using 4-byte integers rather than 8-byte doubles. There was also a floor on regret at $-310,000,000$ for every action. This made it easier to unprune actions that were initially pruned but later improved. This also prevented integer overflows.

Since CFR's average strategy is not guaranteed to converge to a Nash equilibrium in sixplayer poker, there is no theoretical benefit to using the average strategy as opposed to the current strategy. Nevertheless, after the initial 800 minutes of training, we stored the average strategy for the first betting round and used this as the blueprint strategy for the first betting round. For situations after the first betting round, a snapshot of the current strategy was taken every 200 minutes after the initial 800 minutes of training. The blueprint strategy after the first betting round was constructed by averaging together these snapshots (32). Averaging together snapshots that were saved to disk rather than maintaining the average strategy in memory for situations after the first round reduced memory usage by nearly half, and also reduced the computational cost of each MCCFR iteration.

## Further details of the real-time search algorithm

The real-time search component of Pluribus, which determines an improved strategy during actual play, is the most intricate component of the system. On the first betting round, real-time search is used if an opponent chooses a raise size that is more than $\$ 100$ off from any raise size in the blueprint action abstraction and there are no more than four players remaining in the

```
Algorithm 1 MCCFR with Negative-Regret Pruning
The final strategy for infoset $I$ is $\phi(I)$ normalized.
function MCCFR-P(T) $\triangleright$ Conduct External-Sampling Monte Carlo CFR with Pruning
    for $P_{i} \in \mathcal{P}$ do
        for $I_{i} \in \mathcal{I}_{i}$ where $P\left(I_{i}\right)=i$ do
            for $a \in A\left(I_{i}\right)$ do
                $R\left(I_{i}, a\right) \leftarrow 0$
                if betting_round $\left(I_{i}\right)=0$ then
                    $\phi\left(I_{i}, a\right) \leftarrow 0$
    for $t=1$ to $T$ do
        for $P_{i} \in \mathcal{P}$ do
            if $t$ mod Strategy_Interval $=0$ then $\quad \triangleright$ Strategy_Interval $=10,000$ in Pluribus
                Update-Strategy $\left(\emptyset, P_{i}\right)$
            if $t>$ Prune_Threshold then $\triangleright$ Prune_Threshold is 200 minutes in Pluribus
                $q \sim[0,1) \quad \triangleright$ Sample from a uniform random distribution between 0 and 1
                if $q<0.05$ then
                    Traverse-MCCFR $\left(\emptyset, P_{i}\right)$
                else
                    Traverse-MCCFR-P $\left(\emptyset, P_{i}\right)$
            else
                TRaverse-MCCFR( $\left(\emptyset, P_{i}\right)$
        if $t<L C F R \_$Treshold and $t$ mod Discount_Interval $=0$ then
            $d \leftarrow \frac{t / \text { Discount_Interval }_{t / \text { Discount Interval }+1} \quad \triangleright \text { Discount_Interval is } 10 \text { minutes in Pluribus }}{}$
            for $P_{i} \in \mathcal{P}$ do
                for $I_{i} \in \mathcal{I}_{i}$ where $P\left(I_{i}\right)=i$ do
                    for $a \in A\left(I_{i}\right)$ do
                    $R\left(I_{i}, a\right) \leftarrow R\left(I_{i}, a\right) \cdot d$
                    $\phi\left(I_{i}, a\right) \leftarrow \phi\left(I_{i}, a\right) \cdot d$
    return $\phi$
function Calculate-Strategy $\left(R\left(I_{i}\right), I_{i}\right) \quad \triangleright$ Calculates the strategy based on regrets
    sum $\leftarrow 0$
    for $a \in A\left(I_{i}\right)$ do
        sum $\leftarrow \operatorname{sum}+R_{+}\left(I_{i}, a\right)$
    for $a \in A\left(I_{i}\right)$ do
        if sum $>0$ then
            $\sigma\left(I_{i}, a\right) \leftarrow \frac{R_{+}\left(I_{i}, a\right)}{\operatorname{sum}}$
        else
            $\sigma\left(I_{i}, a\right) \leftarrow \frac{1}{\left|A\left(I_{i}\right)\right|}$
    return $\sigma\left(I_{i}\right)$
```

```
function Update-StRategy $\left(h, P_{i}\right)$
                    $\triangleright$ Update the average strategy for $P_{i}$
    if $h$ is terminal or $P_{i}$ not in hand or betting_round $(h)>0$ then
        return $\quad \triangleright$ Average strategy only tracked on first betting round
    else if $h$ is a chance node then
        $a \sim \sigma(h) \quad \triangleright$ Sample an action from the chance probabilities
        Update-Strategy $\left(h \cdot a, P_{i}\right)$
    else if $P(h)=P_{i}$ then
        $I_{i} \leftarrow I_{i}(h) \quad \triangleright$ The $P_{i}$ infoset of this node
        $\sigma\left(I_{i}\right) \leftarrow$ Calculate-Strategy $\left(R\left(I_{i}\right), I_{i}\right) \quad \triangleright$ Determine the strategy at this infoset
        $a \sim \sigma\left(I_{i}\right) \quad \triangleright$ Sample an action from the probability distribution
        $\phi\left(I_{i}, a\right) \leftarrow \phi\left(I_{i}, a\right)+1 \quad \triangleright$ Increment the action counter
        $\operatorname{Update-StRategy}\left(h \cdot a, P_{i}\right)$
    else
        for $a \in A(h)$ do
            $\operatorname{Update-StRatEGY}\left(h \cdot a, P_{i}\right) \quad \triangleright$ Traverse each action
function Traverse-MCCFR $\left(h, P_{i}\right) \quad \triangleright$ Update the regrets for $P_{i}$
    if $h$ is terminal then
        return $u_{i}(h)$
    else if $P_{i}$ not in hand then
        return TRAVERSE-MCCFR $\left(h \cdot 0, P_{i}\right) \quad \triangleright$ The remaining actions are irrelevant to $P_{i}$
    else if $h$ is a chance node then
        $a \sim \sigma(h) \quad \triangleright$ Sample an action from the chance probabilities
        return TRAVERSE-MCCFR $\left(h \cdot a, P_{i}\right)$
    else if $P(h)=P_{i}$ then
        $I_{i} \leftarrow I_{i}(h) \quad \triangleright$ The $P_{i}$ infoset of this node
        $\sigma\left(I_{i}\right) \leftarrow \operatorname{Calculate-StRategy}\left(R\left(I_{i}\right), I_{i}\right) \quad \triangleright$ Determine the strategy at this infoset
        $v(h) \leftarrow 0 \quad \triangleright$ Initialize expected value at zero
        for $a \in A(h)$ do
            $v(h, a) \leftarrow \operatorname{TRAVERSE-MCCFR}\left(h \cdot a, P_{i}\right) \quad \triangleright$ Traverse each action
            $v(h) \leftarrow v(h)+\sigma\left(I_{i}, a\right) \cdot v(h, a) \quad \triangleright$ Update the expected value
        for $a \in A(h)$ do
            $R\left(I_{i}, a\right) \leftarrow R\left(I_{i}, a\right)+v(h, a)-v(h) \quad \triangleright$ Update the regret of each action
        return $v(h) \quad \triangleright$ Return the expected value
    else
        $I_{P(h)} \leftarrow I_{P(h)}(h) \quad \triangleright$ The $P_{P(h)}$ infoset of this node
    $\sigma\left(I_{P(h)}\right) \leftarrow$ Calculate-Strategy $\left(R\left(I_{P(h)}\right), I_{P(h)}\right)$
    $a \sim \sigma\left(I_{P(h)}\right) \quad \triangleright$ Sample an action from the probability distribution
    return Traverse-MCCFR $\left(h \cdot a, P_{i}\right)$
```

```
function TRAVERSE-MCCFR-P $\left(h, P_{i}\right) \quad \triangleright$ MCCFR with pruning for very negative regrets
    if $h$ is terminal then
        return $u_{i}(h)$
    else if $P_{i}$ not in hand then
        return Traverse-MCCFR-P $\left(h \cdot 0, P_{i}\right) \quad \triangleright$ The remaining actions are irrelevant to $P_{i}$
    else if $h$ is a chance node then
        $a \sim \sigma(h) \quad \triangleright$ Sample an action from the chance probabilities
        return TRAVERSE-MCCFR-P $\left(h \cdot a, P_{i}\right)$
    else if $P(h)=P_{i}$ then
        $I_{i} \leftarrow I_{i}(h) \quad \triangleright$ The $P_{i}$ infoset of this node
        $\sigma\left(I_{i}\right) \leftarrow \operatorname{Calculate-StRategy}\left(R\left(I_{i}\right), I_{i}\right) \quad \triangleright$ Determine the strategy at this infoset
        $v(h) \leftarrow 0 \quad \triangleright$ Initialize expected value at zero
        for $a \in A(h)$ do
            if $R\left(I_{i}, a\right)>C$ then $\quad \triangleright C$ is $-300,000,000$ in Pluribus
                $v(h, a) \leftarrow$ TRAVERSE-MCCFR-P $\left(h \cdot a, P_{i}\right)$
                $\operatorname{explored}(a) \leftarrow$ True
                $v(h) \leftarrow v(h)+\sigma\left(I_{i}, a\right) \cdot v(h, a) \quad \triangleright$ Update the expected value
            else
                explored $(a) \leftarrow$ False
        for $a \in A(h)$ do
            if $\operatorname{explored}(a)=$ True then
                $R\left(I_{i}, a\right) \leftarrow R\left(I_{i}, a\right)+v(h, a)-v(h) \quad \triangleright$ Update the regret for this action
        return $v(h) \quad \triangleright$ Return the expected value
    else
        $I_{P(h)} \leftarrow I_{P(h)}(h) \quad \triangleright$ The $P_{P(h)}$ infoset of this node
        $\sigma\left(I_{P(h)}\right) \leftarrow \operatorname{Calculate-StRategy}\left(R\left(I_{P(h)}\right), I_{P(h)}\right)$
        $a \sim \sigma\left(I_{P(h)}\right) \quad \triangleright$ Sample an action from the probability distribution
        return TRAVERSE-MCCFR-P $\left(h \cdot a, P_{i}\right)$
```

hand. Otherwise, Pluribus uses the randomized pseudo-harmonic action translation algorithm (which empirically has the lowest exploitability of all known action translation algorithms) to map the raise to a nearby size and proceeds to play according to the blueprint strategy as if that mapped raise size had been chosen (39). Real-time search is always used to determine the strategy on the second, third, and fourth betting rounds.

## Structure of imperfect-information subgames in Pluribus

In perfect-information games, search begins at a root node, which is the current state of the world. Players are able to change their strategy below the root node until a leaf node of the search space is reached (or a terminal node that ends the game is reached). A leaf node's value is fixed and ideally approximates the value that would ensue if all players played perfectly beyond the leaf node. In practice, this value can be estimated by, for example, using a gamespecific heuristic or by assuming that all players play according to the blueprint strategy after the leaf node. The root, the leaves, and the nodes in between them constitute a subgame. For perfect-information games, if the value of each leaf node equals the value of both players playing perfectly from that point forward and the subgame is solved exactly (i.e., no player can do better given every other player's strategy in the subgame), then the solution to the subgame is part of a Nash equilibrium strategy. Thus, search is a method of achieving in real time a closer approximation of a Nash equilibrium compared to the blueprint strategy.

Pluribus uses a generalized form of this search paradigm that allows it to be run in imperfectinformation games. The two main modifications are how the root of the subgame is represented and how the values at the leaf nodes are calculated.

Since players do not know the exact node they are in when playing an imperfect-information game, it is not possible to have a single root node. Instead, the "root" of a subgame in Pluribus is a probability distribution over the nodes in a public state $G$. The probability of a node is the
normalized probability that that node would be reached assuming that all players play according to a strategy profile $\sigma$. Formally, the root of an imperfect-information subgame in Pluribus is a chance node with outcomes that lead to each node in the root public state. The probability of the chance node outcome leading to node $h$ is $\frac{\pi^{\sigma}(h)}{\sum_{h^{\prime} \in G} \pi^{\sigma}\left(h^{\prime}\right)}$, where $G$ is the root public state.

If the root probability distribution is correct (that is, all players were indeed playing according to $\sigma$ ), then solving the remainder of the game starting at the root public state would produce an optimal strategy going forward (if there are only two players remaining in the hand, or if all remaining players play the same solution). Of course, it is not possible for Pluribus to know the strategy profile $\sigma$ that all players played (and therefore know the true probability distribution over nodes in $G$ ) because Pluribus does not have access to the other players' strategies, only their observed actions.

To handle this problem, Pluribus calculates what its strategy would be in each situation where an opponent has acted, and assumes the opponent followed that strategy. We refer to this as unsafe search (49). Unsafe search lacks theoretical guarantees on performance even in two-player zero-sum games and there are cases where it leads to highly exploitable strategies. While safe search alternatives exist that have provable guarantees on exploitability in two-player zero-sum games (50-52), in practice they tend to do worse than modern, careful unsafe search in head-to-head performance. Unsafe search has the added benefit that it is not necessary to compute a strategy for a hand that has zero probability. In six-player poker, most hands are folded with $100 \%$ probability in the first action, so whenever search is conducted, there is typically only a small fraction of hands for which a strategy actually needs to be computed. This makes unsafe search faster by about a factor of four.

To obtain the practical benefits of unsafe search while mitigating the potential for highly exploitable strategies, we use a new form of nested search in this paper that is similar to nested unsafe search but which always solves starting at the beginning of the current betting round
(described in the body of this paper) rather than starting at the most recent decision point. The traversers strategy is held fixed for actions the traverser has already chosen in the betting round. Experimental results show that if the root public state follows a chance node with a large branching factor, as is always done in this form of nested search, then unsafe search typically produces strategies with low exploitability in two-player zero-sum games (52). This mitigates the potential for unsafe search to be exploitable, while still retaining the practical average benefits of unsafe search.

To implement this form of search, Pluribus maintains a probability distribution over pairs of private cards a player (including Pluribus itself) could be holding from an outside observer's perspective based on the actions that have occurred so far in the game. Since there are $\binom{52}{2}=$ 1326 combinations of private cards that a player can have, each probability is initially $\frac{1}{1326}$. Whenever a round ends, Pluribus makes the public state at the start of the new round the root of the new subgame. It also updates its belief distribution for each player's possible private cards using Bayes' rule based on strategy profile $\sigma$, where $\sigma$ is the blueprint if real-time search has not yet been conducted; otherwise $\sigma$ is the output of the previously-run search.

If search is being done on the first betting round, then the subgame extends to the end of the round, with leaf nodes at the chance nodes at the start of the second round. If search is being done on the second betting round and there were more than two players at the start of the round, then leaf nodes occur either at the chance nodes at the stat of the third betting round or immediately after the second raise action, whichever is earlier. In all other cases, the subgame extends to the end of the game.

Pluribus used one of two different forms of CFR to compute a strategy in the subgame depending on the size of the subgame and the part of the game. If the subgame is relatively large or it is early in the game, then Monte Carlo Linear CFR is used just as it was for the blueprint strategy computation. Otherwise, Pluribus uses an optimized vector-based form of

Linear CFR (38) that samples one set of public board cards per thread (42). In both cases, Pluribus actually plays according to the strategy on the final iteration rather than the weighted average strategy over all iterations. However, $\sigma$ is updated based on the weighted average strategy. Playing according to the final iteration helps Pluribus avoid poor actions that are not completely eliminated in CFR's weighted average strategy. This could potentially come at the cost of increased exploitability, but in practice the final iteration's strategy is sufficiently unpredictable that any exploitation is infeasible.

## Abstraction and off-tree actions in subgames

When conducting search, the current betting round (the round of the root public state) uses lossless information abstraction, but subsequent betting rounds use lossy information abstraction in which each information situation is assigned to one of 500 buckets per round. Each information situation in a bucket is treated identically. Only a small number of possible raise actions are included in the subgame action abstraction, typically no more than five. If an opponent chooses an action that was not included in the action abstraction, then that action is added as a valid action into the subgame model and the subgame is searched again from the root (which is typically the start of the betting round). The root does not change until a new round is reached.

However, there is a risk that the strategy resulting from the second search might be different than the strategy that Pluribus has played so far with its actual hand. For example, there could be actions $A$ and $B$ at the root that are distinct actions but nevertheless the game trees following each are identical, so there is effectively no difference between them. The first time the subgame is searched, the solution might call for Pluribus to always choose $A$ at the root and therefore Pluribus would choose $A$. However, the second time the subgame is searched, the solution might call for Pluribus to always choose $B$ at the root even though Pluribus had already chosen $A$. Since the new solution assumes Pluribus would always choose $B$ at the root and never $A$,
the new strategy beyond $A$ might be nonsensical. Not accounting for this problem could cause Pluribus to choose a poor strategy when search is conducted in later situations.

One way to prevent this would be to freeze the strategy for decision points preceding the current public state that Pluribus is in. That would effectively always make the current public state the root of the subgame. However, freezing the strategy for opponent decision points would make Pluribus less robust to the possibility of an opponent shifting to a different strategy.

Instead, we only freeze the action probabilities for any actions Pluribus chose so far in the subgame. The action probabilities are only frozen for its actual hand, not for its other possible hands. Opponent action probabilities are also not frozen. However, when a betting round ends, the root of the subgame is changed to the start of the new betting round, which effectively freezes the strategy of all infosets preceding the new root public state.

## Leaf node values in imperfect-information subgames

Assuming that all players play according to the blueprint strategy profile following leaf nodes, as is often done in perfect-information games, can lead to highly exploitable strategies when doing search in imperfect-information subgames even if the blueprint is an exact Nash equilibrium because the opponents could shift to different strategies that may exploit the searcher.

To address this in Pluribus, when a leaf node is reached in a subgame during search, each player still in the hand simultaneously chooses one of four different continuation strategies to play for the remainder of the game. They may also choose any mixture of (i.e., probability distribution over) the four strategies. The choice a player makes must be identical for all leaf nodes that the player cannot distinguish among (i.e., it must be identical for all leaf nodes that are in the same infoset). This final choice of strategy for the remainder of the game is essentially just another action in the subgame and is selected via the search algorithm (in the
case of Pluribus, our improved MCCFR algorithm discussed earlier in this paper). $\downarrow^{1}$
In different applications, the set of possible continuations strategies could be chosen in various different ways, and there could be more or less than four of them per player. In Pluribus, the four possible continuation strategies are biased modifications of the blueprint strategy. The first is simply the unaltered blueprint. The second is the blueprint strategy biased toward folding. Specifically, the probability of folding in all decision points is multiplied by 5 and the probabilities are then renormalized. The third is the blueprint strategy biased toward calling by multiplying the call probability by 5 and then renormalizing all the probabilities. The fourth is the blueprint strategy biased toward raising by multiplying all raise action probabilities by 5 and then renormalizing all the probabilities. To reduce memory usage, the continuation strategies are compressed by sampling an action at each abstract infoset according to the probability distribution at that abstract infoset. Rather than storing the regrets or probabilities for each action, only that action is stored (using the minimum number of bits necessary). This is unlikely to bias the results because the odds of encountering the same abstract infoset multiple times is very low.

If an opponent chose an action earlier in the game that was not in the blueprint action abstraction, then a leaf node in the subgame may not exist in the blueprint abstraction. In that case we map the leaf node to the nearest node in the blueprint action abstraction using the deterministic pseudo-harmonic action translation (39).

[^0]```
Algorithm 2 Nested search used in Pluribus
    $I \leftarrow \emptyset \quad \triangleright$ Initialize our current infoset $I$ as the infoset at the start of the game
    $G_{\text {root }} \leftarrow G(I) \quad \triangleright$ Initialize the subgame root as the public node of $I$
    $\sigma \leftarrow \sigma_{\text {blueprint }} \quad \triangleright$ Initialize the game's strategy profile as the blueprint
    function $\operatorname{OPponentTurn}(a) \quad \triangleright$ Opponent chose action $a$
        if ! InAbstraction $(I, a)$ then $\triangleright$ If action $a$ is not already in the action abstraction for infoset $I$
            for each node $h \in G(I)$ do
                $\operatorname{AddAction}(h, a) \quad \triangleright$ Add $a$ as a legal action for all nodes in the public node of $I$
            $\sigma \leftarrow \operatorname{Search}\left(G_{\text {root }}\right) \quad \triangleright$ Compute a new strategy profile for the subgame starting at the root
        $I \leftarrow I \cdot a \quad \triangleright$ Advance the current infoset from $I$ to $I \cdot a$
        CheckNewRound()
```

    function OURTURN $\triangleright$ It is our turn to act
        $a \sim \sigma(I) \quad \triangleright$ Sample an action from the probability distribution at this infoset
        frozen $(I)=$ True $\quad \triangleright \sigma(I)$ won't change if a new strategy is computed for the subgame
        $I \leftarrow I \cdot a \quad \triangleright$ Advance the current infoset from $I$ to $I \cdot a$
        CheckNewRound()
    function CheckNewRound $\triangleright$ Check if a new round is reached. If so, update the root.
        if BettingRound $(G(I))>$ BettingRound $\left(G_{\text {root }}\right)$ then
            $G_{\text {root }}=G(I) \quad \triangleright$ Update the root public node
            $\sigma \leftarrow \operatorname{Search}\left(G_{\text {root }}\right) \quad$ Compute a new strategy profile for the subgame starting at the root
    | Participant Alias | Hands Played | Participant Win Rate | Standard Error |
| :--- | :--- | :---: | ---: |
| Participant A | 9,121 | $141.8 \mathrm{mbb} /$ game | $85.8 \mathrm{mbb} /$ game |
| Participant B | 7,512 | $-86.7 \mathrm{mbb} /$ game | $87.7 \mathrm{mbb} /$ game |
| Participant C | 6,713 | $-49.9 \mathrm{mbb} /$ game | 121.9 mbb /game |
| Participant D | 6,055 | $4.9 \mathrm{mbb} /$ game | $122.2 \mathrm{mbb} /$ game |
| Participant E | 5,510 | $101.5 \mathrm{mbb} /$ game | $121.8 \mathrm{mbb} /$ game |
| Participant F | 4,483 | $-59.7 \mathrm{mbb} /$ game | $108.6 \mathrm{mbb} /$ game |
| Participant G | 2,560 | $-126.9 \mathrm{mbb} /$ game | $133.5 \mathrm{mbb} /$ game |
| Participant H | 2,509 | $229.7 \mathrm{mbb} /$ game | $177.6 \mathrm{mbb} /$ game |
| Participant I | 1,535 | $-131.7 \mathrm{mbb} /$ game | $229.9 \mathrm{mbb} /$ game |
| Participant J | 1,378 | $89.3 \mathrm{mbb} /$ game | $237.6 \mathrm{mbb} /$ game |
| Participant K | 1,365 | $141.0 \mathrm{mbb} /$ game | $284.1 \mathrm{mbb} /$ game |
| Participant L | 771 | $-35.6 \mathrm{mbb} /$ game | $418.6 \mathrm{mbb} /$ game |
| Participant M | 488 | $-492.5 \mathrm{mbb} /$ game | $515.4 \mathrm{mbb} /$ game |

Table S1: The number of hands played, win rate after modest variance reduction, and standard error after modest variance reduction for each human participant in the $5 \mathrm{H}+1 \mathrm{AI}$ experiment. Due to the extremely high variance in no-limit poker and the small sample size, no meaningful conclusions can be drawn about the performance of any individual participant. Only Pluribus's performance after the application of AIVAT can be meaningfully evaluated.

## References and Notes

1. D. Billings, A. Davidson, J. Schaeffer, D. Szafron, The challenge of poker. Artif. Intell. 134, 201-240 (2002). doi:10.1016/S0004-3702(01)00130-8
2. J. von Neumann, Zur Theorie der Gesellschaftsspiele. Math. Ann. 100, 295-320 (1928). doi:10.1007/BF01448847
3. J. Nash, Non-Cooperative Games. Ann. Math. 54, 286 (1951). doi:10.2307/1969529
4. M. Bowling, N. Burch, M. Johanson, O. Tammelin, Computer science. Heads-up limit hold'em poker is solved. Science 347, 145-149 (2015). doi:10.1126/science. 1259433 Medline
5. M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, M. Bowling, DeepStack: Expert-level artificial intelligence in heads-up nolimit poker. Science 356, 508-513 (2017). doi:10.1126/science.aam6960 Medline
6. N. Brown, T. Sandholm, Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science 359, 418-424 (2018). doi:10.1126/science.aao1733 Medline
7. J. Schaeffer, One Jump Ahead: Challenging Human Supremacy in Checkers (Springer-Verlag, New York, 1997).
8. M. Campbell, A. J. Hoane Jr., F.-H. Hsu, Deep Blue. Artif. Intell. 134, 57-83 (2002). doi:10.1016/S0004-3702(01)00129-1
9. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis, Mastering the game of Go with deep neural networks and tree search. Nature 529, 484-489 (2016). doi:10.1038/nature 16961 Medline
10. Recently, in the real-time strategy games Dota 2 (20) and StarCraft 2 (21), AIs have beaten top humans, but as humans have gained more experience against the AIs, humans have learned to beat them. This may be because for those two-player zero-sum games, the AIs were generated by techniques not guaranteed to converge to a Nash equilibrium, so they do not have the unbeatability property that Nash equilibruim strategies have in two-player zero-sum games. (Dota 2 involves two teams of five players each. However, because the players on the same team have the same objective and are not limited in their communication, the game is two-player zero-sum from an AI and game-theoretic perspective).
11. S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) (2011), pp. 533-540.
12. S. Ganzfried, T. Sandholm, ACM Trans. Econ. Comp. (TEAC) 3, 8 (2015). Best of EC-12 special issue.
13. C. Daskalakis, P. W. Goldberg, C. H. Papadimitriou, The Complexity of Computing a Nash Equilibrium. SIAM J. Comput. 39, 195-259 (2009). doi:10.1137/070699652
14. X. Chen, X. Deng, S.-H. Teng, Settling the complexity of computing two-player Nash equilibria. J. Assoc. Comput. Mach. 56, 14 (2009). doi:10.1145/1516512.1516516
15. A. Rubinstein, Inapproximability of Nash Equilibrium. SIAM J. Comput. 47, 917-959 (2018). doi:10.1137/15M1039274
16. K. Berg, T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2017).
17. M. A. Zinkevich, M. Bowling, M. Wunder, The lemonade stand game competition: Solving unsolvable puzzles. ACM SIGecom Exchanges 10, 35-38 (2011). doi:10.1145/1978721.1978730
18. G. Tesauro, Temporal difference learning and TD-Gammon. Commun. ACM 38, 58-68 (1995). doi:10.1145/203330.203343
19. D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, D. Hassabis, Mastering the game of Go without human knowledge. Nature 550, 354-359 (2017). doi:10.1038/nature24270 Medline
20. A. I. Open, A. I. Open, Five, https://blog.openai.com/openai-five/ (2018).
21. O. Vinyals et al., AlphaStar: Mastering the Real-Time Strategy Game StarCraft II, https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/ (2019).
22. L. S. Shapley, Advances in Game Theory, M. Drescher, L. S. Shapley, A. W. Tucker, Eds. (Princeton Univ. Press, 1964).
23. R. Gibson, Regret minimization in games and the development of champion multiplayer computer poker-playing agents, Ph.D. thesis, University of Alberta (2014).
24. T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2015), pp. 4127-4131. Senior Member Track.
25. T. Sandholm, Computer science. Solving imperfect-information games. Science 347, 122123 (2015). doi:10.1126/science.aaa4614 Medline
26. M. Johanson, N. Burch, R. Valenzano, M. Bowling, in International Conference on Autonomous Agents and Multiagent Systems (AAMAS) (2013), pp. 271-278.
27. S. Ganzfried, T. Sandholm, in AAAI Conference on Artificial Intelligence (AAAI) (2014), pp. 682-690.
28. N. Brown, S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and Multiagent Systems (AAMAS) (2015), pp. 7-15.
29. M. Zinkevich, M. Johanson, M. H. Bowling, C. Piccione, in Neural Information Processing Systems (NeurIPS) (2007), pp. 1729-1736.
30. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information (2013).
31. M. B. Johanson, Robust strategies and counter-strategies: from superhuman to optimal play, Ph.D. thesis, University of Alberta (2016).
32. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information (2016).
33. N. Brown, T. Sandholm, in International Joint Conference on Artificial Intelligence (IJCAI) (2016), pp. 4238-4239.
34. E. G. Jackson, AAAI Workshop on Computer Poker and Imperfect Information Games (2017).
35. M. Lanctot, K. Waugh, M. Zinkevich, in M. Bowling, Neural Information Processing Systems (NeurIPS) (2009), pp. 1078-1086.
36. M. Johanson, N. Bard, M. Lanctot, R. Gibson, M. Bowling, in International Conference on Autonomous Agents and Multiagent Systems (AAMAS) (2012), pp. 837-846.
37. R. Gibson, M. Lanctot, N. Burch, D. Szafron, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2012), pp. 1355-1361.
38. N. Brown, T. Sandholm, AAAI Conference on Artificial Intelligence (AAAI) (2019).
39. S. Ganzfried, T. Sandholm, in International Joint Conference on Artificial Intelligence (IJCAI) (2013), pp. 120-128.
40. Here we use term subgame the way it is usually used in AI. In game theory that word is used differently by requiring a subgame to start with a node where the player whose turn it is to move has no uncertainty about state-in particular, no uncertainty about the opponents' private information.
41. N. Brown, T. Sandholm, B. Amos, in Neural Information Processing Systems (NeurIPS) (2018), pp. 7663-7674.
42. M. Johanson, K. Waugh, M. Bowling, M. Zinkevich, in International Joint Conference on Artificial Intelligence (IJCAI) (2011), pp. 258-265.
43. E. P. DeBenedictis, Rebooting Computers as Learning Machines. Computer 49, 84-87 (2016). doi:10.1109/MC.2016.156
44. N. Burch, M. Schmid, M. Moravcik, D. Morill, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2018), pp. 949-956.
45. Due to the presence of AIVAT and because the players did not know each others' scores during the experiment, there was no incentive for the players to play a risk-averse or riskseeking strategy in order to outperform the other human.
46. A. Gilpin, T. Sandholm, Lossless abstraction of imperfect information games. J. Assoc. Comput. Mach. 54, 25 (2007). doi:10.1145/1284320.1284324
47. K. Waugh, AAAI Workshop on Computer Poker and Imperfect Information (2013).
48. A. Gilpin, T. Sandholm, T. B. Sørensen, in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) (2007), pp. 50-57.
49. S. Ganzfried, T. Sandholm, in International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) (2015), pp. 37-45.
50. N. Burch, M. Johanson, M. Bowling, in AAAI Conference on Artificial Intelligence (AAAI) (2014), pp. 602-608.
51. M. Moravcik, M. Schmid, K. Ha, M. Hladik, S. Gaukrodger, in AAAI Conference on Artificial Intelligence (AAAI) (2016), pp. 572-578.
52. N. Brown, T. Sandholm, in Neural Information Processing Systems (NeurIPS) (2017), pp. 689-699.

[^0]:    ${ }^{1}$ We used a similar technique in the two-player NLTH poker AI Modicum (41), but in that case only the opponent chose among the continuation strategies while the traversing player could only choose the blueprint strategy. That is sound in theory for two-player zero-sum games, but in practice gives more power to the opponent and therefore results in the traversing player playing a more conservative, defensive strategy that has lower expected value. Allowing the traversing player to also choose among continuation strategies helps balance the strength of the players and helps address this problem. This is also sound in theory for two-player zero-sum games.

